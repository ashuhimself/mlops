{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Testing S3 (MinIO) Connection from Jupyter\n",
      "==================================================\n",
      "\n",
      "1. Environment Configuration:\n",
      "   AWS_ACCESS_KEY_ID: Not set\n",
      "   AWS_SECRET_ACCESS_KEY: Not set\n",
      "   AWS_ENDPOINT_URL: Not set\n",
      "   MLFLOW_TRACKING_URI: Not set\n",
      "\n",
      "2. Creating S3 client...\n",
      "   ‚úÖ S3 client created successfully!\n",
      "\n",
      "3. Listing buckets...\n",
      "   Available buckets:\n",
      "     - features\n",
      "     - mlflow\n",
      "     - models\n",
      "\n",
      "4. Creating test data...\n",
      "   Test data created:\n",
      "          timestamp  value category\n",
      "2024-01-01 00:00:00      0        A\n",
      "2024-01-01 01:00:00      1        B\n",
      "2024-01-01 02:00:00      2        A\n",
      "2024-01-01 03:00:00      3        B\n",
      "2024-01-01 04:00:00      4        A\n",
      "\n",
      "5. Saving data to S3...\n",
      "   ‚úÖ Data saved to s3://features/jupyter_test/test_data_20250727_161908.csv\n",
      "\n",
      "6. Reading data from S3...\n",
      "   ‚úÖ Data read successfully!\n",
      "   First 3 rows:\n",
      "          timestamp  value category\n",
      "2024-01-01 00:00:00      0        A\n",
      "2024-01-01 01:00:00      1        B\n",
      "2024-01-01 02:00:00      2        A\n",
      "\n",
      "7. Listing objects in bucket...\n",
      "   Objects in s3://features/jupyter_test/:\n",
      "     - jupyter_test/test_data_20250727_161847.csv (Size: 145 bytes)\n",
      "     - jupyter_test/test_data_20250727_161908.csv (Size: 145 bytes)\n",
      "\n",
      "==================================================\n",
      "‚úÖ All S3 connection tests passed!\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"Test S3 (MinIO) Connection from Jupyter\"\"\"\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"Testing S3 (MinIO) Connection from Jupyter\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Display configuration\n",
    "print(\"\\n1. Environment Configuration:\")\n",
    "print(f\"   AWS_ACCESS_KEY_ID: {os.environ.get('AWS_ACCESS_KEY_ID', 'Not set')}\")\n",
    "print(f\"   AWS_SECRET_ACCESS_KEY: {'***' if os.environ.get('AWS_SECRET_ACCESS_KEY') else 'Not set'}\")\n",
    "print(f\"   AWS_ENDPOINT_URL: {os.environ.get('AWS_ENDPOINT_URL', 'Not set')}\")\n",
    "print(f\"   MLFLOW_TRACKING_URI: {os.environ.get('MLFLOW_TRACKING_URI', 'Not set')}\")\n",
    "\n",
    "# Create S3 client\n",
    "print(\"\\n2. Creating S3 client...\")\n",
    "try:\n",
    "    s3_client = boto3.client(\n",
    "        's3',\n",
    "        endpoint_url=os.environ.get('AWS_ENDPOINT_URL', 'http://minio:9000'),\n",
    "        aws_access_key_id=os.environ.get('AWS_ACCESS_KEY_ID', 'minio'),\n",
    "        aws_secret_access_key=os.environ.get('AWS_SECRET_ACCESS_KEY', 'minio123')\n",
    "    )\n",
    "    print(\"   ‚úÖ S3 client created successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Error creating S3 client: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# List buckets\n",
    "print(\"\\n3. Listing buckets...\")\n",
    "try:\n",
    "    buckets = s3_client.list_buckets()\n",
    "    print(\"   Available buckets:\")\n",
    "    for bucket in buckets['Buckets']:\n",
    "        print(f\"     - {bucket['Name']}\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Error listing buckets: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Create test data\n",
    "print(\"\\n4. Creating test data...\")\n",
    "test_data = pd.DataFrame({\n",
    "    'timestamp': pd.date_range(start='2024-01-01', periods=5, freq='H'),\n",
    "    'value': range(5),\n",
    "    'category': ['A', 'B', 'A', 'B', 'A']\n",
    "})\n",
    "print(\"   Test data created:\")\n",
    "print(test_data.to_string(index=False))\n",
    "\n",
    "# Save to S3\n",
    "print(\"\\n5. Saving data to S3...\")\n",
    "try:\n",
    "    csv_buffer = test_data.to_csv(index=False)\n",
    "    key = f\"jupyter_test/test_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "    \n",
    "    s3_client.put_object(\n",
    "        Bucket='features',\n",
    "        Key=key,\n",
    "        Body=csv_buffer\n",
    "    )\n",
    "    print(f\"   ‚úÖ Data saved to s3://features/{key}\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Error saving to S3: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Read from S3\n",
    "print(\"\\n6. Reading data from S3...\")\n",
    "try:\n",
    "    response = s3_client.get_object(Bucket='features', Key=key)\n",
    "    df_from_s3 = pd.read_csv(response['Body'])\n",
    "    print(\"   ‚úÖ Data read successfully!\")\n",
    "    print(\"   First 3 rows:\")\n",
    "    print(df_from_s3.head(3).to_string(index=False))\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Error reading from S3: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# List objects\n",
    "print(\"\\n7. Listing objects in bucket...\")\n",
    "try:\n",
    "    objects = s3_client.list_objects_v2(Bucket='features', Prefix='jupyter_test/')\n",
    "    if 'Contents' in objects:\n",
    "        print(\"   Objects in s3://features/jupyter_test/:\")\n",
    "        for obj in objects['Contents'][-5:]:  # Show last 5 objects\n",
    "            print(f\"     - {obj['Key']} (Size: {obj['Size']} bytes)\")\n",
    "    else:\n",
    "        print(\"   No objects found\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Error listing objects: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"‚úÖ All S3 connection tests passed!\")\n",
    "print(\"=\" * 50) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration completed!\n",
      "MLflow tracking URI: http://mlflow:5001\n",
      "Dataset shape: (1000, 6)\n",
      "\n",
      "First 5 rows:\n",
      "    feature_1  feature_2  feature_3 category           timestamp     target\n",
      "0  107.450712   0.366602  21.906881        A 2024-01-01 00:00:00  51.889656\n",
      "1   97.926035   0.220898   3.672136        A 2024-01-01 01:00:00  50.159429\n",
      "2  109.715328   2.023568  10.802575        A 2024-01-01 02:00:00  61.273821\n",
      "3  122.845448   2.451590  33.886065        B 2024-01-01 03:00:00  63.505707\n",
      "4   96.487699   0.064191  80.258568        B 2024-01-01 04:00:00  58.068972\n",
      "‚úÖ Raw data saved to s3://features/data/raw/synthetic_data_20250727_162018.parquet\n",
      "Features shape after encoding: (1000, 10)\n",
      "\n",
      "Feature columns:\n",
      "['feature_1', 'feature_2', 'feature_3', 'category_A', 'category_B', 'category_C', 'category_D', 'hour', 'day_of_week', 'is_weekend']\n",
      "Using experiment: jupyter_ml_experiment (ID: 2)\n",
      "Training Random Forest model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/27 16:20:19 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE: 2.74\n",
      "Test RMSE: 6.02\n",
      "Train R¬≤: 0.925\n",
      "Test R¬≤: 0.682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ MLflow run completed: da8d72bb1ba34d3b9fe510b9dd360d01\n",
      "üèÉ View run rf_model_jupyter at: http://mlflow:5001/#/experiments/2/runs/da8d72bb1ba34d3b9fe510b9dd360d01\n",
      "üß™ View experiment at: http://mlflow:5001/#/experiments/2\n",
      "\n",
      "Recent MLflow runs:\n",
      "                          run_id                       start_time  metrics.test_rmse  metrics.test_r2\n",
      "da8d72bb1ba34d3b9fe510b9dd360d01 2025-07-27 16:20:18.924000+00:00            6.02333         0.682454\n",
      "\n",
      "Objects in s3://features/:\n",
      "  - data/raw/synthetic_data_20250727_162018.parquet (50970 bytes)\n",
      "  - jupyter_test/test_data_20250727_161847.csv (145 bytes)\n",
      "  - jupyter_test/test_data_20250727_161908.csv (145 bytes)\n",
      "  - processed/aggregated_data_2025-07-27.csv (168 bytes)\n",
      "  - raw/sample_data_2025-07-27.csv (3079 bytes)\n",
      "\n",
      "Objects in s3://models/:\n",
      "  - predictions/rf_predictions_20250727_162018.csv (11060 bytes)\n",
      "\n",
      "==================================================\n",
      "‚úÖ MLOps workflow completed successfully!\n",
      "==================================================\n",
      "\n",
      "Next steps:\n",
      "1. View experiment in MLflow UI: http://localhost:5001\n",
      "2. Browse S3 objects in MinIO: http://localhost:9001\n",
      "3. Create Airflow DAG to automate this workflow\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "MLOps Examples Notebook\n",
    "=======================\n",
    "This notebook contains examples of using:\n",
    "1. MinIO S3 for data storage\n",
    "2. MLflow for experiment tracking\n",
    "3. Data processing pipelines\n",
    "4. Model training and tracking\n",
    "\"\"\"\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import os\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 1. Setup Configuration\n",
    "\n",
    "# Configure S3 client\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    endpoint_url='http://minio:9000',\n",
    "    aws_access_key_id='minio',\n",
    "    aws_secret_access_key='minio123'\n",
    ")\n",
    "\n",
    "# Configure MLflow\n",
    "mlflow.set_tracking_uri('http://mlflow:5001')\n",
    "os.environ['MLFLOW_S3_ENDPOINT_URL'] = 'http://minio:9000'\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = 'minio'\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = 'minio123'\n",
    "\n",
    "print(\"Configuration completed!\")\n",
    "print(f\"MLflow tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2. Generate Sample Dataset\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'feature_1': np.random.normal(100, 15, n_samples),\n",
    "    'feature_2': np.random.exponential(2, n_samples),\n",
    "    'feature_3': np.random.uniform(0, 100, n_samples),\n",
    "    'category': np.random.choice(['A', 'B', 'C', 'D'], n_samples),\n",
    "    'timestamp': pd.date_range(start='2024-01-01', periods=n_samples, freq='H')\n",
    "})\n",
    "\n",
    "# Create target variable with some relationship to features\n",
    "data['target'] = (\n",
    "    0.5 * data['feature_1'] + \n",
    "    2.0 * data['feature_2'] + \n",
    "    0.1 * data['feature_3'] +\n",
    "    np.random.normal(0, 5, n_samples)\n",
    ")\n",
    "\n",
    "print(\"Dataset shape:\", data.shape)\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(data.head())\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3. Save Data to S3\n",
    "\n",
    "# Save raw data to S3\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "raw_data_key = f\"data/raw/synthetic_data_{timestamp}.parquet\"\n",
    "\n",
    "# Convert to parquet and upload\n",
    "data.to_parquet('/tmp/temp_data.parquet', index=False)\n",
    "with open('/tmp/temp_data.parquet', 'rb') as f:\n",
    "    s3_client.put_object(\n",
    "        Bucket='features',\n",
    "        Key=raw_data_key,\n",
    "        Body=f\n",
    "    )\n",
    "\n",
    "print(f\"‚úÖ Raw data saved to s3://features/{raw_data_key}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 4. Feature Engineering\n",
    "\n",
    "# One-hot encode categorical features\n",
    "data_encoded = pd.get_dummies(data, columns=['category'], prefix='category')\n",
    "\n",
    "# Create time-based features\n",
    "data_encoded['hour'] = data_encoded['timestamp'].dt.hour\n",
    "data_encoded['day_of_week'] = data_encoded['timestamp'].dt.dayofweek\n",
    "data_encoded['is_weekend'] = (data_encoded['day_of_week'] >= 5).astype(int)\n",
    "\n",
    "# Drop timestamp for modeling\n",
    "features = data_encoded.drop(['timestamp', 'target'], axis=1)\n",
    "target = data_encoded['target']\n",
    "\n",
    "print(\"Features shape after encoding:\", features.shape)\n",
    "print(\"\\nFeature columns:\")\n",
    "print(features.columns.tolist())\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 5. Train Model with MLflow Tracking\n",
    "\n",
    "# Create or get experiment\n",
    "experiment_name = \"jupyter_ml_experiment\"\n",
    "experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "if experiment is None:\n",
    "    experiment_id = mlflow.create_experiment(\n",
    "        experiment_name,\n",
    "        tags={\"environment\": \"jupyter\", \"project\": \"mlops_demo\"}\n",
    "    )\n",
    "else:\n",
    "    experiment_id = experiment.experiment_id\n",
    "\n",
    "print(f\"Using experiment: {experiment_name} (ID: {experiment_id})\")\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features, target, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train model with MLflow tracking\n",
    "with mlflow.start_run(experiment_id=experiment_id, run_name=\"rf_model_jupyter\"):\n",
    "    # Log parameters\n",
    "    n_estimators = 100\n",
    "    max_depth = 10\n",
    "    min_samples_split = 5\n",
    "    \n",
    "    mlflow.log_param(\"n_estimators\", n_estimators)\n",
    "    mlflow.log_param(\"max_depth\", max_depth)\n",
    "    mlflow.log_param(\"min_samples_split\", min_samples_split)\n",
    "    mlflow.log_param(\"n_features\", X_train.shape[1])\n",
    "    mlflow.log_param(\"n_samples_train\", X_train.shape[0])\n",
    "    \n",
    "    # Train model\n",
    "    print(\"Training Random Forest model...\")\n",
    "    rf_model = RandomForestRegressor(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        random_state=42\n",
    "    )\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_train = rf_model.predict(X_train)\n",
    "    y_pred_test = rf_model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "    train_r2 = r2_score(y_train, y_pred_train)\n",
    "    test_r2 = r2_score(y_test, y_pred_test)\n",
    "    \n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"train_rmse\", train_rmse)\n",
    "    mlflow.log_metric(\"test_rmse\", test_rmse)\n",
    "    mlflow.log_metric(\"train_r2\", train_r2)\n",
    "    mlflow.log_metric(\"test_r2\", test_r2)\n",
    "    \n",
    "    print(f\"Train RMSE: {train_rmse:.2f}\")\n",
    "    print(f\"Test RMSE: {test_rmse:.2f}\")\n",
    "    print(f\"Train R¬≤: {train_r2:.3f}\")\n",
    "    print(f\"Test R¬≤: {test_r2:.3f}\")\n",
    "    \n",
    "    # Create and log feature importance plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': features.columns,\n",
    "        'importance': rf_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False).head(10)\n",
    "    \n",
    "    sns.barplot(data=feature_importance, x='importance', y='feature')\n",
    "    plt.title('Top 10 Feature Importances')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('/tmp/feature_importance.png')\n",
    "    mlflow.log_artifact('/tmp/feature_importance.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Log the model\n",
    "    mlflow.sklearn.log_model(\n",
    "        rf_model, \n",
    "        \"random_forest_model\",\n",
    "        input_example=X_train.iloc[:5]\n",
    "    )\n",
    "    \n",
    "    # Save predictions to S3\n",
    "    predictions_df = pd.DataFrame({\n",
    "        'actual': y_test,\n",
    "        'predicted': y_pred_test,\n",
    "        'error': y_test - y_pred_test\n",
    "    })\n",
    "    \n",
    "    pred_key = f\"predictions/rf_predictions_{timestamp}.csv\"\n",
    "    predictions_csv = predictions_df.to_csv(index=False)\n",
    "    s3_client.put_object(\n",
    "        Bucket='models',\n",
    "        Key=pred_key,\n",
    "        Body=predictions_csv\n",
    "    )\n",
    "    mlflow.log_param(\"predictions_s3_path\", f\"s3://models/{pred_key}\")\n",
    "    \n",
    "    run_id = mlflow.active_run().info.run_id\n",
    "    print(f\"\\n‚úÖ MLflow run completed: {run_id}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 6. List Recent Experiments and Runs\n",
    "\n",
    "# List recent runs\n",
    "runs = mlflow.search_runs(experiment_ids=[experiment_id], max_results=5)\n",
    "print(\"\\nRecent MLflow runs:\")\n",
    "print(runs[['run_id', 'start_time', 'metrics.test_rmse', 'metrics.test_r2']].to_string(index=False))\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 7. List S3 Objects Created\n",
    "\n",
    "# List objects in features bucket\n",
    "print(\"\\nObjects in s3://features/:\")\n",
    "response = s3_client.list_objects_v2(Bucket='features', MaxKeys=10)\n",
    "if 'Contents' in response:\n",
    "    for obj in response['Contents'][-5:]:\n",
    "        print(f\"  - {obj['Key']} ({obj['Size']} bytes)\")\n",
    "\n",
    "# List objects in models bucket\n",
    "print(\"\\nObjects in s3://models/:\")\n",
    "response = s3_client.list_objects_v2(Bucket='models', MaxKeys=10)\n",
    "if 'Contents' in response:\n",
    "    for obj in response['Contents'][-5:]:\n",
    "        print(f\"  - {obj['Key']} ({obj['Size']} bytes)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"‚úÖ MLOps workflow completed successfully!\")\n",
    "print(\"=\"*50)\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. View experiment in MLflow UI: http://localhost:5001\")\n",
    "print(\"2. Browse S3 objects in MinIO: http://localhost:9001\")\n",
    "print(\"3. Create Airflow DAG to automate this workflow\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
